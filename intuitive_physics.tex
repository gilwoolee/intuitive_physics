\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:

\usepackage[nonatbib, final]{nips_2017}

% to comp„ÖÅle a camera-ready version, add the [final] option, e.g.:
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath, amssymb, amsfonts, amsthm}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\bibliographystyle{ieeetr}

\title{Bayesian-Adaptive Deep Reinforcement Learning via Ensemble Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{Gilwoo Lee \\
  \texttt{gilwoo@cs.uw.edu} \\
  %% examples of more authors
  \And
  Jeongseok Lee \\
  \texttt{jslee02@cs.uw.edu}
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Introduction}
While reinforcement learning is capable of controlling complex autonomous systems, RL algorithms typically require huge amounts of data and can overfit to a particular task or to be prone to disturbance. One of main challenges that needs to be addressed is train a policy robust to various model uncertainties and disturbances. In this project, we aim to address this challenge via an ensemble policy for Bayes-Adaptive Reinforcement Learning~\cite{ghavamzadeh2015bayesian}.

We assume that there exists a latent physics variable $\phi$ which determines the transition function of the underlying MDP, i.e. the transition function  $P(s',\phi' |s, \phi, a)$ is now a function of state, action, and $\phi$. We would like to learn a policy which maximizes the long term reward given $\phi$. Formally, this is called Bayes-Adaptive MDP~\cite{ghavamzadeh2015bayesian, guez2012efficient}, defined by a tuple $<\mathcal{S}', \mathcal{A}, P', P_0', R'>$ where
\begin{itemize}
\item $\mathcal{S'} = \mathcal{S}\times \Phi$ is the set of (states, physics variable),
\item $\mathcal{A}$ is the set of actions,
\item $P(\cdot|s, \phi, a)$ is the transition function between hyper-states, conditioned
on action a being taken in hyper-state $(s, \phi)$,
\item $P_0\in \mathcal{P}(\mathcal{S} \times \Phi)$ combines the initial distribution over hyper-states,
\item $R'(s, \phi, a)$ represents the reward obtained when action $a$ is
taken in hyper-state $(s,\phi)$.
\end{itemize}

We would like to find the optimal policy for the following Bellman equaton:
\begin{equation}\label{eq:rl}
V^*(s, \phi) = \max_a \mathbb{E} \bigg[R(s, a, \phi) + \gamma \sum_{s', \phi'}P(s',\phi'|s, \phi, a)V^*(s', \phi') \bigg]
\end{equation}
This formualtion is often refered as Bayes-Adaptive Reinforcement Learning (BARL)~\cite{ghavamzadeh2015bayesian}.

We make two simplifications to BARL formulation. First, we assume that the dynamics of $s'$ and $\phi'$ are independent given $P(s, \phi, a)$, i.e.
\begin{equation*}
P(s',\phi'|s, \phi, a) = P(s'|s, \phi, a)\cdot P(\phi'|s, \phi, a).
\end{equation*}
Second, we assume that $\phi$ changes slowly w.r.t. the system such that an optimal policy for a fixed $\phi,$
$\pi_{\phi}$, is a reasonable short-term approximation of the long-term optimal policy.

Above two assumptions allow us to simplify BARL with a gated ensemble policy learning method. At the high-level, we have a gating network that determines the best estimate of the physics parameters at time $t$,
\begin{align*}
P(\phi_t) = g(s_{t-1}, \phi_{t-1}, a_{t-1})
\end{align*}
which serves as a gating function for an ensemble of $\phi$-dependent policies, i.e.
\begin{align*}
 \pi &= \argmax_{\pi} \mathbb{E}_{\phi \sim b(\phi)} \bigg[R(s, a, \phi) + \gamma \sum_{s', \phi'}P(s',\phi'|s, \phi, a)V^*(s', \phi') \bigg]
\end{align*}

We model $g$ as a network capable of modeling evolving state change, e.g. Recurrent Neural Networks or Temporal Convolutional Networks. At the low level, we train an ensemble of $N$ policies, where each policy is trained with $\phi$ sampled from the distribution of physics parameters this system may encounter during the course of operation.

\section{Background}
Our work is closely related to QMDP~\cite{littman1995learning, karkus2017qmdp} which is an approximation for POMDP. QMDP approximates POMDP by assuming fully-observable MDP after 1-step, and approximating the Q-value at the current belief state $b(s)$ as $Q_a(b) =\sum_s b(s)Q_{MDP}(s, a)$. In our problem setup, we have a belief over the physics parameters $\phi$ of the MDP, $b(\phi)$, and we compute the policy $Q_a(s;b) = \sum_\phi b(\phi)Q_{MDP}(s,a;\phi)$.

The BAMDP formulation is also similar to POMDP formulation used in POMDP-lite~\cite{chen2016pomdp} which assumes that the hidden state variables are are constant or only change deterministically. In our case, the hidden state variables correspond to the physics parameters $\phi$. The authors of POMDP-lite have shown that such formulation is ``equivalent to a set of fully observable Markov decision processes indexed by a hidden parameter''~\cite{chen2016pomdp}, which, in our case, could be viewed as a discretization of $\phi$.


\bibliography{intuitive_physics}

\end{document}
